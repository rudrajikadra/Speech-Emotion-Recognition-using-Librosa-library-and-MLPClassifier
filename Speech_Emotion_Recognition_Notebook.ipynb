{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Speech Emotion Recognition Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-G_Y2V88K0X",
        "colab_type": "text"
      },
      "source": [
        "# Speech Emotion Recognition using Librosa\n",
        "\n",
        "\n",
        "#### RAVDESS Dataset\n",
        "This is the Ryerson Audio-Visual Database of Emotional Speech and Song dataset, and is free to download. This dataset has 7356 files rated by 247 individuals 10 times on emotional validity, intensity, and genuineness. The entire dataset is 24.8GB from 24 actors.\n",
        "\n",
        "Dataset on Google Drive: https://drive.google.com/file/d/1wWsrN2Ep7x6lWqOXfr4rpKGYrJhWc8z7/view"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byp8AEbuxqys",
        "colab_type": "code",
        "outputId": "c73af761-8546-415f-bb28-2754ca36fdf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#Connect your Drive with Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMcSXcrCyUHB",
        "colab_type": "code",
        "outputId": "814c9540-b976-45c6-bad5-a8250dd1cea8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Check where your Dataset Zip File is\n",
        "!ls '/content/drive/My Drive/Important Extras/Data Science Works/_Data Science Work/Speech Emotion Recognition'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Speech Emotion Recognition Notebook.ipynb'\n",
            " speech-emotion-recognition-ravdess-data.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQOY709my-fv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Unzip the file contents\n",
        "!unzip '/content/drive/My Drive/Important Extras/Data Science Works/_Data Science Work/Speech Emotion Recognition/speech-emotion-recognition-ravdess-data.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9dIRJbovpGI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9b4d88bf-8796-4388-cb1a-b169534272e9"
      },
      "source": [
        "#You can see the zip folder has been extracted\n",
        "!ls"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actor_01  Actor_05  Actor_09  Actor_13\tActor_17  Actor_21  drive\n",
            "Actor_02  Actor_06  Actor_10  Actor_14\tActor_18  Actor_22  sample_data\n",
            "Actor_03  Actor_07  Actor_11  Actor_15\tActor_19  Actor_23\n",
            "Actor_04  Actor_08  Actor_12  Actor_16\tActor_20  Actor_24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utFTMFyG4fdC",
        "colab_type": "code",
        "outputId": "db68d0e4-9817-40ce-995c-a04bb3fe6596",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "#Install Librosa and SoundFile to your Machine\n",
        "!pip install librosa soundfile"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.6/dist-packages (0.6.3)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.6/dist-packages (0.10.3.post1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.22.2.post1)\n",
            "Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.2.2)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.18.4)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.38.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.48.0)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (2.1.8)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.6/dist-packages (from librosa) (0.15.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile) (1.14.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa) (0.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba>=0.38.0->librosa) (46.4.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile) (2.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZTfgUu51RCX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import All Important Libraries\n",
        "import librosa\n",
        "import soundfile\n",
        "import os, glob, pickle\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvGpG07E_3Uo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function for extracting mfcc, chroma, and mel features from sound file\n",
        "def extract_feature(file_name, mfcc, chroma, mel):\n",
        "  with soundfile.SoundFile(file_name) as sound_file:\n",
        "    X = sound_file.read(dtype=\"float32\")\n",
        "    sample_rate=sound_file.samplerate\n",
        "    if chroma:\n",
        "      stft=np.abs(librosa.stft(X))\n",
        "    result=np.array([])\n",
        "    if mfcc:\n",
        "      mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
        "      result=np.hstack((result, mfccs))\n",
        "    if chroma:\n",
        "      chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
        "      result=np.hstack((result, chroma))\n",
        "    if mel:\n",
        "      mel=np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
        "      result=np.hstack((result, mel))\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-UIAg03Xis1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define the motions dictionary\n",
        "emotions = {\n",
        "    '01':'neutral',\n",
        "    '02':'calm',\n",
        "    '03':'happy',\n",
        "    '04':'sad',\n",
        "    '05':'angry',\n",
        "    '06':'fearful',\n",
        "    '07':'disgust',\n",
        "    '08':'surprised'\n",
        "}\n",
        "\n",
        "#Emotions we want to observe\n",
        "observed_emotions = ['calm', 'happy', 'fearful', 'disgust']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "154OQUcVZ4tt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load the data and extract features for each sound file\n",
        "def load_data(test_size = 0.2):\n",
        "  x, y = [], []\n",
        "  for folder in glob.glob('/content/Actor_*'):\n",
        "    print(folder)\n",
        "    for file in glob.glob(folder + '/*.wav'):\n",
        "      file_name = os.path.basename(file)\n",
        "      emotion = emotions[file_name.split('-')[2]]\n",
        "      if emotion not in observed_emotions:\n",
        "        continue\n",
        "      feature = extract_feature(file, mfcc = True, chroma = True, mel = True)\n",
        "      x.append(feature)\n",
        "      y.append(emotion)\n",
        "  return train_test_split(np.array(x), y, test_size = test_size, random_state = 9)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2f0qX1PjBU0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "ea97dc04-3154-479b-918f-51a83cbbf2f3"
      },
      "source": [
        "x_train,x_test,y_train,y_test=load_data(test_size=0.2)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Actor_01\n",
            "/content/Actor_13\n",
            "/content/Actor_03\n",
            "/content/Actor_04\n",
            "/content/Actor_05\n",
            "/content/Actor_18\n",
            "/content/Actor_07\n",
            "/content/Actor_02\n",
            "/content/Actor_14\n",
            "/content/Actor_21\n",
            "/content/Actor_08\n",
            "/content/Actor_15\n",
            "/content/Actor_17\n",
            "/content/Actor_10\n",
            "/content/Actor_16\n",
            "/content/Actor_11\n",
            "/content/Actor_09\n",
            "/content/Actor_06\n",
            "/content/Actor_24\n",
            "/content/Actor_20\n",
            "/content/Actor_23\n",
            "/content/Actor_12\n",
            "/content/Actor_19\n",
            "/content/Actor_22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpN02uwkpekV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "78905251-85ce-4d73-92aa-ce7e334840e3"
      },
      "source": [
        "#Shape of train and test set and Number of features extracted\n",
        "print((x_train.shape[0], x_test.shape[0]))\n",
        "print(f'Features extracted: {x_train.shape[1]}')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(614, 154)\n",
            "Features extracted: 180\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXwY7HVDuuTv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initialise Multi Layer Perceptron Classifier\n",
        "model = MLPClassifier(alpha = 0.01, batch_size = 256, epsilon = 1e-08, hidden_layer_sizes = (300,), learning_rate = 'adaptive', max_iter = 500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_LKbQ3KvJy-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "429e189c-625a-4187-d6e8-d729e85783ff"
      },
      "source": [
        "model.fit(x_train, y_train)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.01, batch_size=256, beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(300,), learning_rate='adaptive',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=500,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
              "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glTuS50TwN0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Predict for the test set\n",
        "y_pred = model.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqVnMdQmwUXG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "20d90e48-b15b-47f3-ff7c-62151a3dff82"
      },
      "source": [
        "#Calculate Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 75.97%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouC4y9tzxRKt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}